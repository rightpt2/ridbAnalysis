{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we doing here\n",
    "\n",
    "### Things to do\n",
    "<ol>\n",
    "    <li>Read differnet info from CSVs</li>\n",
    "    <li>Create quick data dictionary</li>\n",
    "    <li>Clean the data</li>\n",
    "    <li>Decide what we import to the DB</li>\n",
    "    <li>Load data into amazon postgres database</li>\n",
    "    <ol style=\"list-style-type: lower-alpha; padding-bottom: 0;\">\n",
    "      <li style=\"margin-left:1em\">How to connect / start a session</li>\n",
    "      <li style=\"margin-left:1em\">How to define schema</li>\n",
    "      <li style=\"margin-left:1em\">How to create a table</li>\n",
    "      <li style=\"margin-left:1em; padding-bottom: 0;\">How to load data</li>\n",
    "     </ol>\n",
    "    <li>Write some good queries</li>\n",
    "    <li>Create an ETL of Occupancy using jupyter</li> \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"Reservations/*.csv\")\n",
    "files = files\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RIDB segments the sales data by year, lets check to make sure it has the same info in each file\n",
    "columns_data = {}\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, nrows=10)\n",
    "    columns_data[file] = df.columns.values\n",
    "\n",
    "# after printing the columns_data we see that it has the same columns in each sales file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets write some quick functions to process our data\n",
    "\n",
    "def read_data(file):\n",
    "    print(file)\n",
    "    return pd.read_csv(file, low_memory=False)\n",
    "\n",
    "def trim_select_data(df):\n",
    "    \"\"\"quick funciton to clean up our data\"\"\"\n",
    "    \n",
    "    #first trim to only cols we care about\n",
    "    tgt_cols = ['HistoricalReservationID', 'OrderNumber', 'Agency', 'OrgID',\n",
    "       'CodeHierarchy', 'RegionCode', 'RegionDescription',\n",
    "       'ParentLocationID', 'ParentLocation', 'LegacyFacilityID', 'Park',\n",
    "       'SiteType', 'UseType', 'ProductID', 'EntityType', 'EntityID',\n",
    "       'FacilityID', 'FacilityZIP', 'FacilityState', 'FacilityLongitude',\n",
    "       'FacilityLatitude', 'CustomerZIP', 'CustomerState',\n",
    "       'CustomerCountry', 'TotalPaid', 'StartDate', 'EndDate', 'OrderDate',\n",
    "       'NumberOfPeople']\n",
    "\n",
    "    df = df.loc[:,tgt_cols]\n",
    "\n",
    "    # next rename the data in the data frame\n",
    "    col_names = ['historical_reservation_id', 'order_number', 'agency', 'orgid',\n",
    "           'code_hierarchy', 'region_code', 'region_description',\n",
    "           'parent_location_id', 'parent_location', 'legacy_facility_id',\n",
    "           'park', 'site_type', 'use_type', 'product_id', 'entity_type',\n",
    "           'entity_id', 'facility_id', 'facility_zip', 'facility_state',\n",
    "           'facility_longitude', 'facility_latitude', 'customer_zip',\n",
    "           'customer_state', 'customer_country', 'total_paid', 'start_date',\n",
    "           'end_date', 'order_date', 'number_of_people']\n",
    "\n",
    "    #rename the columns\n",
    "    df.columns = col_names\n",
    "\n",
    "    #Lets only select overnight stays at campsites\n",
    "    df = df.loc[df.use_type == \"Overnight\",:]\n",
    "    df = df.loc[df['entity_type'] == 'Site', :]\n",
    "    \n",
    "    # We will need to coerce the datas into datetime as some of the data isnt clean\n",
    "    for x in [\"start_date\", \"end_date\", \"order_date\"]:\n",
    "        df[x] = pd.to_datetime(df[x],errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    \n",
    "    # for some reasons some of these reservations dont have facility ids, we will replace with -1 as a flag\n",
    "    df.facility_id = df.facility_id.fillna(-1).astype(int)\n",
    "    df.entity_id = df.entity_id.fillna(-1).astype(int)    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(files[2])\n",
    "df = trim_select_data(df)\n",
    "df.shape\n",
    "\n",
    "# for some reasons some transactions are here multiple times. Lets remove the duplicates\n",
    "df.drop_duplicates(subset=['order_number'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets just do this for California - 7 parks\n",
    "df_CA = df.loc[df.facility_state == 'CA',:]\n",
    "\n",
    "# now lets load this into a list of dictionaries that we can call into our data loader functions\n",
    "records = df_CA.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_test = records[0:100]\n",
    "records_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Some helpful tutorials on SQLAlchemy and loading data</h3>\n",
    "<a href=\"https://www.freecodecamp.org/news/sqlalchemy-makes-etl-magically-easy-ab2bd0df928/\">Fee Code Camp</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the sqlalchemy module, create the table, then load up our data\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, Numeric, Boolean, DateTime, BigInteger\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# After creating the table we want with sqlalchemy orm I realiezed it is a bit tough to tune the uploads. \n",
    "# We will revert to psycog2 to load in the data\n",
    "\n",
    "# local postgresql://postgres:79zDvTF9zHfTNJoVQ@localhost/ridb_local\n",
    "# amazon -  postgresql://postgres:XoMDmLymgj4XaJsv8BWS@ridbinfo.cxokcdt2wl4r.us-east-2.rds.amazonaws.com/ridb\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:79zDvTF9zHfTNJoVQ@localhost/ridb_local\", echo = False) #Update with credientials\n",
    "Base = declarative_base()\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "\n",
    "meta = MetaData()\n",
    "\n",
    "reservations = Table(\n",
    "    \"reservations\", meta,\n",
    "    Column('order_number', String, primary_key=True),\n",
    "    Column('historical_reservation_id',Numeric),\n",
    "    Column('agency',String),\n",
    "    Column('orgid',Numeric),\n",
    "    Column('code_hierarchy',String),\n",
    "    Column('region_code',String),\n",
    "    Column('region_description',String),\n",
    "    Column('parent_location_id',Numeric),\n",
    "    Column('parent_location',String),\n",
    "    Column('legacy_facility_id',Numeric),\n",
    "    Column('park',String),\n",
    "    Column('site_type',String),\n",
    "    Column('use_type',String),\n",
    "    Column('product_id',Numeric),\n",
    "    Column('entity_type',String),\n",
    "    Column('entity_id',Numeric),    \n",
    "    Column('facility_id',Numeric),\n",
    "    Column('facility_zip',String),\n",
    "    Column('facility_state',String),\n",
    "    Column('facility_longitude',Numeric),\n",
    "    Column('facility_latitude',Numeric),\n",
    "    Column('customer_zip',String),\n",
    "    Column('customer_state',String),\n",
    "    Column('customer_country',String),\n",
    "    Column('total_paid',Numeric),\n",
    "    Column('start_date',DateTime),\n",
    "    Column('end_date',DateTime),\n",
    "    Column('order_date',DateTime),\n",
    "    Column('number_of_people',Numeric)\n",
    ")\n",
    "\n",
    "meta.create_all(engine)\n",
    "\n",
    "session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use psycopg2 \n",
    "\n",
    "import psycopg2\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    host=\"localhost\", \n",
    "    dbname=\"ridb_local\", \n",
    "    user=\"postgres\", \n",
    "    password=\"79zDvTF9zHfTNJoVQ\")\n",
    "\n",
    "connection.autocommit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick check that all of the id's are unique\n",
    "ids = [x['historical_reservation_id'] for x in records_test]\n",
    "len(ids) == len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time   0.01042\n",
      "Memory 0.03125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.778115  , 0.38208647, 0.37040987, ..., 0.0552389 , 0.55048104,\n",
       "        0.60423926],\n",
       "       [0.93198505, 0.56147375, 0.4599968 , ..., 0.23433635, 0.82224727,\n",
       "        0.66307169],\n",
       "       [0.94452589, 0.63188879, 0.54454422, ..., 0.83007989, 0.08258352,\n",
       "        0.51736673],\n",
       "       ...,\n",
       "       [0.20833203, 0.31038416, 0.32296924, ..., 0.21050959, 0.024795  ,\n",
       "        0.81911979],\n",
       "       [0.30692169, 0.16071502, 0.51024851, ..., 0.10784099, 0.36420807,\n",
       "        0.47930508],\n",
       "       [0.92921302, 0.15964683, 0.73651769, ..., 0.58580998, 0.20868611,\n",
       "        0.81116431]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timerfunc import profile\n",
    "\n",
    "\n",
    "@profile\n",
    "def new_array():\n",
    "    # lets test this with a large array\n",
    "    arr = np.random.rand(300,100)\n",
    "    return arr\n",
    "\n",
    "new_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@profile\n",
    "def one_by_one(connection, records):\n",
    "    with connection.cursor() as cursor:\n",
    "        i=1\n",
    "        for reservation in records:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO reservations VALUES (\n",
    "                   %(historical_reservation_id)s,\n",
    "                    %(order_number)s,\n",
    "                    %(agency)s,\n",
    "                    %(orgid)s,\n",
    "                    %(code_hierarchy)s,\n",
    "                    %(region_code)s,\n",
    "                    %(region_description)s,\n",
    "                    %(parent_location_id)s,\n",
    "                    %(parent_location)s,\n",
    "                    %(legacy_facility_id)s,\n",
    "                    %(park)s,\n",
    "                    %(site_type)s,\n",
    "                    %(use_type)s,\n",
    "                    %(product_id)s,\n",
    "                    %(entity_type)s,\n",
    "                    %(entity_id)s,\n",
    "                    %(facility_id)s,\n",
    "                    %(facility_zip)s,\n",
    "                    %(facility_state)s,\n",
    "                    %(facility_longitude)s,\n",
    "                    %(facility_latitude)s,\n",
    "                    %(customer_zip)s,\n",
    "                    %(customer_state)s,\n",
    "                    %(customer_country)s,\n",
    "                    %(total_paid)s,\n",
    "                    %(start_date)s,\n",
    "                    %(end_date)s,\n",
    "                    %(order_date)s,\n",
    "                    %(number_of_people)s\n",
    "                );\n",
    "            \"\"\", reservation)\n",
    "            i += 1\n",
    "        print(i)\n",
    "\n",
    "one_by_one(connection=connection, records=records_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_execute_batch(connection=connection, records=records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok so lets see if we can find the list of all camsites at a given entity\n",
    "att_files = glob('RIDB_attributes/*')\n",
    "att_files\n",
    "\n",
    "# ok so steps are, read the file, see if there are any names that include camp, write list to dictionary plus key fields\n",
    "\n",
    "def tgt_col_find(file_lst, tgt_col='campsiteid'):\n",
    "\n",
    "    #lets create the dict we are going to return\n",
    "    dict_out = {}\n",
    "    \n",
    "    #now we are going to iterate over the list of files, read the into pandas and check to see if they have the tgt col\n",
    "    for file in file_lst:\n",
    "        df_working = pd.read_csv(file, nrows=10)\n",
    "        cols = [x for x in df_working.columns.values if tgt_col.lower() in x.lower()]\n",
    "        if len(cols) > 0:\n",
    "            dict_out[file] = cols\n",
    "    \n",
    "    return dict_out\n",
    "\n",
    "tgt_col_find(att_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campsite_info = pd.read_csv('RIDB_attributes/Campsites_API_v1.csv')\n",
    "campsite_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Now about that data Dictionary\n",
    "\n",
    "<a href=\"https://ridb.recreation.gov/docs#/Facilities/getFacility\">RIDB Data defintions for API</a> \n",
    "<br>This is kinda helpful but no info about reservations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets understand facilities and campsite Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob(\"RIDB_attributes/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.read_csv(\"RIDB_attributes/Facilities_API_v1.csv\")\n",
    "df_f.iloc[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('RIDB_attributes/Campsites_API_v1.csv')\n",
    "\n",
    "df_c.shape[0] - df_c.groupby(['FacilityID','CampsiteID']).head(1).shape[0] # so these are unique - great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f['FacilityID'] = df_f['FacilityID'].astype(str)\n",
    "df_c['FacilityID'] = df_c['FacilityID'].astype(str)\n",
    "\n",
    "\n",
    "check = pd.merge(left=df_f, right=df_c, how='outer', on=['FacilityID'])\n",
    "\n",
    "sum((check['CampsiteID'].notnull())) / check.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We know about Campsites and Facilities\n",
    "### Lets make some tables in our Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the sqlalchemy modual and learn how some of this works\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, Numeric, Boolean, DateTime\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try a test for a local database \n",
    "engine = create_engine(\"postgresql://postgres:79zDvTF9zHfTNJoVQ@localhost/dvdrental\")\n",
    "df = pd.read_sql(sql = \"\"\"SELECT * FROM actor\"\"\", con=engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try connecting to RIDB db local and create our first table\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:79zDvTF9zHfTNJoVQ@localhost/ridb\", echo=False)\n",
    "conn = engine.connect()\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "## looks like the declartive_base can be used to create tables\n",
    "class Campground(Base):\n",
    "    __tablename__ = \"Campground\"\n",
    "    FacilityCampsiteID = Column(String(1000), primary_key=True)\n",
    "    CampsiteID = Column(String(50))\n",
    "    FacilityID = Column(String(50))\n",
    "    CampsiteName = Column(String)\n",
    "    CampsiteType = Column(String)\n",
    "    TypeOfUse = Column(String)\n",
    "    Loop = Column(String)\n",
    "    CampsiteAccessible = Column(Boolean)\n",
    "    CampsiteLongitude = Column(Numeric)\n",
    "    CampsiteLatitude = Column(Numeric)\n",
    "    CreatedDate = Column(DateTime)\n",
    "    LastUpdatedDate = Column(DateTime)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"(id='%s', Date='%s', Type='%s', Value='%s')\" % (self.id, self.Date, self.Type, self.Value)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "Campground.__table__.create(bind=engine, checkfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets clean up the campground_csv so it matches what we want it to\n",
    "print(df_c.dtypes)\n",
    "\n",
    "# a quick formula to make this faster\n",
    "def col_trans(df, col_name, datatype):\n",
    "    df[col_name] = df[col_name].astype(datatype)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = col_trans(df_c, 'CampsiteID', str)\n",
    "df_c = col_trans(df_c, 'FacilityID', str)\n",
    "df_c['CreatedDate'] = pd.to_datetime(df_c['CreatedDate'], errors='coerce', format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we use some sql_alchemy magic to create that data in the db\n",
    "# # Set up of the table in db and the file to import\n",
    "# fileToRead = 'file.csv'\n",
    "# tableToWriteTo = 'tableName'\n",
    "\n",
    "# # Panda to create a lovely dataframe\n",
    "# df_to_be_written = pd.read_csv(fileToRead)\n",
    "# # The orient='records' is the key of this, it allows to align with the format mentioned in the doc to insert in bulks.\n",
    "# listToWrite = df_to_be_written.to_dict(orient='records')\n",
    "\n",
    "# metadata = sqlalchemy.schema.MetaData(bind=engine,reflect=True)\n",
    "# table = sqlalchemy.Table(tableToWriteTo, metadata, autoload=True)\n",
    "\n",
    "# # Open the session\n",
    "# Session = sessionmaker(bind=engine)\n",
    "# session = Session()\n",
    "\n",
    "# # Inser the dataframe into the database in one bulk\n",
    "# conn.execute(table.insert(), listToWrite)\n",
    "\n",
    "# # Commit the changes\n",
    "# session.commit()\n",
    "\n",
    "# # Close the session\n",
    "# session.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Facilites(Base):\n",
    "    __tablename__ = \"Facilities\"\n",
    "    FacilityID = Column(String(50), primary_key=True)\n",
    "    LegacyFacilityID = Column(String(50))\n",
    "    OrgFacilityID = Column(String(50))\n",
    "    ParentOrgID = Column(String(50))\n",
    "    ParentRecAreaID = Column(String(50))\n",
    "    FacilityName = Column(String(500))\n",
    "    FacilityDescription = Column(String)\n",
    "    FacilityTypeDescription = Column(String)\n",
    "    FacilityUseFeeDescription = Column(String)\n",
    "    FacilityDirections = Column(String)\n",
    "    FacilityPhone = Column(String)\n",
    "    FacilityReservationURL = Column(String)\n",
    "    FacilityMapURL = Column(String)\n",
    "    FacilityAdaAccess = Column(String)\n",
    "    FacilityLongitude = Column(Numeric)\n",
    "    FacilityLatitude = Column(Numeric)\n",
    "    Keywords = Column(String)\n",
    "    StayLimit = Column(Numeric)\n",
    "    Reservable = Column(Boolean)\n",
    "    Enabled = Column(Boolean)\n",
    "    LastUpdatedDate = Column(DateTime)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Facilities.__table__.create(bind=engine, checkfirst=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaration of the class in order to write into the database. This structure is standard and should align with SQLAlchemy's doc.\n",
    "class Current(Base):\n",
    "    __tablename__ = 'tableName'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Date = Column(String(500))\n",
    "    Type = Column(String(500))\n",
    "    Value = Column(Numeric())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(id='%s', Date='%s', Type='%s', Value='%s')\" % (self.id, self.Date, self.Type, self.Value)\n",
    "\n",
    "# Set up of the table in db and the file to import\n",
    "fileToRead = 'file.csv'\n",
    "tableToWriteTo = 'tableName'\n",
    "\n",
    "# Panda to create a lovely dataframe\n",
    "df_to_be_written = pd.read_csv(fileToRead)\n",
    "# The orient='records' is the key of this, it allows to align with the format mentioned in the doc to insert in bulks.\n",
    "listToWrite = df_to_be_written.to_dict(orient='records')\n",
    "\n",
    "metadata = sqlalchemy.schema.MetaData(bind=engine,reflect=True)\n",
    "table = sqlalchemy.Table(tableToWriteTo, metadata, autoload=True)\n",
    "\n",
    "# Open the session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Inser the dataframe into the database in one bulk\n",
    "conn.execute(table.insert(), listToWrite)\n",
    "\n",
    "# Commit the changes\n",
    "session.commit()\n",
    "\n",
    "# Close the session\n",
    "session.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(files[0], encoding= 'unicode_escape', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['FacilityZIP'] = df2['FacilityZIP'].astype(str)\n",
    "df2['CustomerZIP'] = df2['CustomerZIP'].astype(str)\n",
    "df2['StartDate'] = pd.to_datetime(df2['StartDate'], errors='coerce', format='%Y-%m-%d')\n",
    "df2['EndDate'] = pd.to_datetime(df2['EndDate'], errors='coerce', format='%Y-%m-%d')\n",
    "df2['EndDate'] =df2['EndDate'].fillna(df2['StartDate'])\n",
    "df2['CustomerState'] = df2['CustomerState'].astype(str)\n",
    "df2['CustomerCountry'] = df2['CustomerCountry'].astype(str)\n",
    "df2['OrderDate'] = pd.to_datetime(df2['OrderDate'], errors='coerce', format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = pd.DataFrame(df2.dtypes, columns=['dtype'])\n",
    "obj = data_cols['dtype'][1]\n",
    "data_cols['dtype'] == obj\n",
    "\n",
    "txt_cols = list(data_cols.loc[data_cols['dtype'] == obj,:].index.values)\n",
    "txt_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = {}\n",
    "\n",
    "for column in txt_cols:\n",
    "    print(column)\n",
    "    data_len[column] = [df2[column].map(len).min(), df2[column].map(len).max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_data['RIDB_Reservations/2008.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpful tutorial \n",
    "\n",
    "<a href=\"https://www.dataquest.io/blog/loading-data-into-postgres/\">Load Data</a> \n",
    "to local database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
